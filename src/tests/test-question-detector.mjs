#!/usr/bin/env node\n/**\n * Test Suite for QuestionDetector Class\n * \n * Comprehensive test suite covering:\n * - Basic question detection\n * - Pattern recognition\n * - Semantic analysis\n * - Question classification\n * - Performance validation\n * - Real-world scenarios\n */\n\nimport {fileURLToPath} from 'url'\nimport {dirname, join} from 'path'\nimport {QuestionDetector, QuestionDetectionConfig} from '../src/services/question-detector.js'\n\nconst __filename = fileURLToPath(import.meta.url)\nconst __dirname = dirname(__filename)\n\n// Test configuration\nconst testConfig: Partial<QuestionDetectionConfig> = {\n  confidenceThreshold: 0.7,\n  enablePatternMatching: true,\n  enableSemanticAnalysis: true,\n  enableContextAnalysis: false,\n  enableQuestionClassification: true,\n  classificationDepth: 'detailed',\n  enableCaching: true,\n  cacheSize: 100\n}\n\n// Test data sets\nconst questionSamples = {\n  factual: [\n    'What is the capital of France?',\n    'Who invented the telephone?',\n    'When was the first moon landing?',\n    'Where is the Statue of Liberty located?',\n    'What time is it?',\n    'Who is the current president?'\n  ],\n  \n  procedural: [\n    'How do I install Node.js?',\n    'How can I fix this error?',\n    'What steps should I follow to deploy my app?',\n    'How do you make a sandwich?',\n    'Can you show me how to set up the database?',\n    'What is the process for creating a new user account?'\n  ],\n  \n  causal: [\n    'Why is the sky blue?',\n    'Why did the server crash?',\n    'What causes global warming?',\n    'Why do we need to use async/await?',\n    'What is the reason for this behavior?',\n    'Why does this function return undefined?'\n  ],\n  \n  comparative: [\n    'Which is better, React or Vue?',\n    'What is the difference between let and var?',\n    'Which approach should I choose?',\n    'Is MySQL better than PostgreSQL?',\n    'Which one is more efficient?',\n    'What are the pros and cons of each option?'\n  ],\n  \n  confirmatory: [\n    'Is this correct?',\n    'Do you agree?',\n    'Can we proceed with this plan?',\n    'Should I commit these changes?',\n    'Are you sure about this?',\n    'Is it working properly?'\n  ],\n  \n  hypothetical: [\n    'What would happen if we increased the memory limit?',\n    'Could this approach work?',\n    'Would it be possible to integrate with their API?',\n    'What if we used a different framework?',\n    'Could you help me with this problem?',\n    'Would this solution scale well?'\n  ],\n  \n  conversational: [\n    'How are you doing today?',\n    'What do you think about this weather?',\n    'Did you have a good weekend?',\n    'Are you enjoying this project?',\n    'What are your plans for tonight?',\n    'How was your lunch?'\n  ],\n  \n  complex: [\n    'What is the best way to implement real-time collaboration features in a web application, and how should I handle conflicts when multiple users edit the same document simultaneously?',\n    'Can you explain the difference between microservices and monolithic architecture, and in what scenarios would you choose one over the other, considering factors like team size, deployment complexity, and scalability requirements?',\n    'How do I set up a CI/CD pipeline that automatically tests, builds, and deploys my application to multiple environments while ensuring zero downtime, and what tools would you recommend for monitoring and rollback capabilities?'\n  ]\n}\n\nconst nonQuestions = [\n  'The weather is nice today.',\n  'I think this solution will work.',\n  'Please submit the report by Friday.',\n  'The server is running on port 3000.',\n  'Thank you for your help.',\n  'Good morning everyone.',\n  'Let me know when you\\'re ready.',\n  'The meeting is scheduled for 2 PM.',\n  'Here are the latest updates.',\n  'I will review this later.'\n]\n\n// Edge cases\nconst edgeCases = [\n  '?', // Just question mark\n  'What?', // Single word question\n  'How about we go to lunch?', // Suggestion phrased as question\n  'I wonder if this is correct.', // Indirect question\n  'Tell me about your project.', // Imperative but seeking information\n  'Really?!', // Emotional response\n  'The question is: what should we do next?', // Embedded question\n  'Are you kidding me???', // Multiple question marks\n  'What the heck is going on here?', // Colloquial question\n  'Is it just me, or does this seem wrong?', // Tag question pattern\n]\n\nclass TestRunner {\n  private detector: QuestionDetector\n  private testResults: {\n    passed: number\n    failed: number\n    total: number\n    failures: Array<{test: string, error: string}>\n  }\n\n  constructor() {\n    this.detector = new QuestionDetector(testConfig)\n    this.testResults = {\n      passed: 0,\n      failed: 0,\n      total: 0,\n      failures: []\n    }\n  }\n\n  async runAllTests(): Promise<void> {\n    console.log('üöÄ Starting QuestionDetector Test Suite\\n')\n    \n    try {\n      await this.detector.initialize()\n      console.log('‚úÖ QuestionDetector initialized successfully\\n')\n    } catch (error) {\n      console.error('‚ùå Failed to initialize QuestionDetector:', error)\n      return\n    }\n\n    // Run test categories\n    await this.testBasicQuestionDetection()\n    await this.testQuestionClassification()\n    await this.testNonQuestionDetection()\n    await this.testEdgeCases()\n    await this.testPerformance()\n    await this.testCaching()\n    await this.testContextAnalysis()\n    await this.testConfigurationUpdates()\n    await this.testBatchProcessing()\n    await this.testRealWorldScenarios()\n\n    this.printResults()\n  }\n\n  private async testBasicQuestionDetection(): Promise<void> {\n    console.log('üìù Testing Basic Question Detection...')\n    \n    const allQuestions = Object.values(questionSamples).flat()\n    \n    for (const question of allQuestions) {\n      await this.runTest(`Basic Detection: \"${question}\"`, async () => {\n        const result = await this.detector.detectQuestion(question)\n        \n        if (!result) {\n          throw new Error('Question not detected')\n        }\n        \n        if (!result.isQuestion) {\n          throw new Error('isQuestion flag is false')\n        }\n        \n        if (result.confidence < testConfig.confidenceThreshold!) {\n          throw new Error(`Confidence too low: ${result.confidence}`)\n        }\n      })\n    }\n    \n    console.log('  ‚úÖ Basic question detection tests completed\\n')\n  }\n\n  private async testQuestionClassification(): Promise<void> {\n    console.log('üìù Testing Question Classification...')\n    \n    for (const [expectedType, questions] of Object.entries(questionSamples)) {\n      for (const question of questions) {\n        await this.runTest(`Classification: \"${question}\" -> ${expectedType}`, async () => {\n          const result = await this.detector.detectQuestion(question)\n          \n          if (!result) {\n            throw new Error('Question not detected')\n          }\n          \n          // Allow some flexibility in classification due to overlapping patterns\n          const isAcceptableClassification = \n            result.questionType === expectedType ||\n            (expectedType === 'conversational' && ['factual', 'confirmatory'].includes(result.questionType)) ||\n            (expectedType === 'factual' && ['conversational', 'procedural'].includes(result.questionType)) ||\n            (expectedType === 'confirmatory' && ['factual', 'conversational'].includes(result.questionType))\n          \n          if (!isAcceptableClassification) {\n            console.warn(`    ‚ö†Ô∏è  Classification mismatch: expected ${expectedType}, got ${result.questionType}`)\n          }\n        })\n      }\n    }\n    \n    console.log('  ‚úÖ Question classification tests completed\\n')\n  }\n\n  private async testNonQuestionDetection(): Promise<void> {\n    console.log('üìù Testing Non-Question Detection...')\n    \n    for (const statement of nonQuestions) {\n      await this.runTest(`Non-Question: \"${statement}\"`, async () => {\n        const result = await this.detector.detectQuestion(statement)\n        \n        if (result && result.isQuestion && result.confidence > testConfig.confidenceThreshold!) {\n          throw new Error(`Non-question incorrectly detected as question (confidence: ${result.confidence})`)\n        }\n      })\n    }\n    \n    console.log('  ‚úÖ Non-question detection tests completed\\n')\n  }\n\n  private async testEdgeCases(): Promise<void> {\n    console.log('üìù Testing Edge Cases...')\n    \n    for (const edgeCase of edgeCases) {\n      await this.runTest(`Edge Case: \"${edgeCase}\"`, async () => {\n        const result = await this.detector.detectQuestion(edgeCase)\n        \n        // Edge cases should either be detected correctly or handled gracefully\n        if (result && result.confidence > 0.95 && !result.isQuestion) {\n          throw new Error('High confidence non-question detection seems incorrect')\n        }\n        \n        // Validate that the result has required fields if it's detected as a question\n        if (result && result.isQuestion) {\n          if (!result.questionType || !result.intent || !result.complexity) {\n            throw new Error('Question analysis incomplete')\n          }\n        }\n      })\n    }\n    \n    console.log('  ‚úÖ Edge case tests completed\\n')\n  }\n\n  private async testPerformance(): Promise<void> {\n    console.log('üìù Testing Performance...')\n    \n    const testText = 'What is the most efficient way to implement a real-time chat application with WebSockets?'\n    const iterations = 100\n    const startTime = performance.now()\n    \n    await this.runTest('Performance Test', async () => {\n      for (let i = 0; i < iterations; i++) {\n        await this.detector.detectQuestion(testText)\n      }\n      \n      const endTime = performance.now()\n      const totalTime = endTime - startTime\n      const averageTime = totalTime / iterations\n      \n      console.log(`    üìä Performance metrics:`)\n      console.log(`       - ${iterations} iterations in ${totalTime.toFixed(2)}ms`)\n      console.log(`       - Average time per detection: ${averageTime.toFixed(2)}ms`)\n      \n      if (averageTime > testConfig.maxAnalysisDelay!) {\n        throw new Error(`Performance too slow: ${averageTime.toFixed(2)}ms > ${testConfig.maxAnalysisDelay}ms`)\n      }\n    })\n    \n    console.log('  ‚úÖ Performance tests completed\\n')\n  }\n\n  private async testCaching(): Promise<void> {\n    console.log('üìù Testing Caching...')\n    \n    const testQuestion = 'How do I configure the database connection?'\n    \n    await this.runTest('Caching Test', async () => {\n      // Clear metrics for clean test\n      this.detector.clearCache()\n      \n      // First call - should not be cached\n      const result1 = await this.detector.detectQuestion(testQuestion)\n      const metrics1 = this.detector.getMetrics()\n      \n      // Second call - should be cached\n      const result2 = await this.detector.detectQuestion(testQuestion)\n      const metrics2 = this.detector.getMetrics()\n      \n      if (!result1 || !result2) {\n        throw new Error('Question not detected in caching test')\n      }\n      \n      if (result1.confidence !== result2.confidence) {\n        throw new Error('Cached result differs from original')\n      }\n      \n      if (metrics2.cacheHits <= metrics1.cacheHits) {\n        throw new Error('Cache hit counter not incremented')\n      }\n      \n      console.log(`    üìä Cache metrics:`)\n      console.log(`       - Cache hits: ${metrics2.cacheHits}`)\n      console.log(`       - Total analyzed: ${metrics2.totalAnalyzed}`)\n    })\n    \n    console.log('  ‚úÖ Caching tests completed\\n')\n  }\n\n  private async testContextAnalysis(): Promise<void> {\n    console.log('üìù Testing Context Analysis...')\n    \n    // Create detector with context enabled for this test\n    const contextDetector = new QuestionDetector({\n      ...testConfig,\n      enableContextAnalysis: true\n    })\n    \n    await contextDetector.initialize()\n    \n    await this.runTest('Context Analysis Test', async () => {\n      // Ask a series of related questions\n      const questions = [\n        'What is React?',\n        'How do I install it?',\n        'Can I use it with TypeScript?',\n        'What about the performance implications?'\n      ]\n      \n      const results: any[] = []\n      \n      for (const question of questions) {\n        const result = await contextDetector.detectQuestion(question, true) // Use context\n        results.push(result)\n      }\n      \n      const context = contextDetector.getContext()\n      \n      if (context.previousQuestions.length !== questions.length) {\n        throw new Error(`Context not properly maintained: expected ${questions.length} questions, got ${context.previousQuestions.length}`)\n      }\n      \n      console.log(`    üìä Context metrics:`)\n      console.log(`       - Questions in context: ${context.previousQuestions.length}`)\n      console.log(`       - Entities tracked: ${context.relatedEntities.length}`)\n    })\n    \n    contextDetector.destroy()\n    console.log('  ‚úÖ Context analysis tests completed\\n')\n  }\n\n  private async testConfigurationUpdates(): Promise<void> {\n    console.log('üìù Testing Configuration Updates...')\n    \n    await this.runTest('Configuration Update Test', async () => {\n      const originalConfig = this.detector.exportAnalysisData().config\n      \n      // Update configuration\n      this.detector.updateConfig({\n        confidenceThreshold: 0.9,\n        enableCaching: false\n      })\n      \n      const updatedConfig = this.detector.exportAnalysisData().config\n      \n      if (updatedConfig.confidenceThreshold !== 0.9) {\n        throw new Error('Configuration update failed for confidenceThreshold')\n      }\n      \n      if (updatedConfig.enableCaching !== false) {\n        throw new Error('Configuration update failed for enableCaching')\n      }\n      \n      // Test that the new config affects detection\n      const testQuestion = 'Is this working?'\n      const result = await this.detector.detectQuestion(testQuestion)\n      \n      // With higher threshold, some marginal questions might not be detected\n      console.log(`    üìä Config test result: ${result ? 'detected' : 'not detected'} (confidence: ${result?.confidence || 0})`)\n      \n      // Restore original configuration\n      this.detector.updateConfig(originalConfig)\n    })\n    \n    console.log('  ‚úÖ Configuration update tests completed\\n')\n  }\n\n  private async testBatchProcessing(): Promise<void> {\n    console.log('üìù Testing Batch Processing...')\n    \n    await this.runTest('Batch Processing Test', async () => {\n      const testTexts = [\n        'What is the weather today?',\n        'I think it will rain.',\n        'How should I prepare?',\n        'The forecast looks good.',\n        'Can we go outside?'\n      ]\n      \n      const results = await this.detector.detectQuestionsBatch(testTexts)\n      \n      if (results.length !== testTexts.length) {\n        throw new Error(`Batch processing returned wrong number of results: ${results.length} vs ${testTexts.length}`)\n      }\n      \n      const questionCount = results.filter(r => r && r.isQuestion).length\n      console.log(`    üìä Batch processing results:`)\n      console.log(`       - Texts processed: ${results.length}`)\n      console.log(`       - Questions detected: ${questionCount}`)\n      console.log(`       - Statements detected: ${results.length - questionCount}`)\n    })\n    \n    console.log('  ‚úÖ Batch processing tests completed\\n')\n  }\n\n  private async testRealWorldScenarios(): Promise<void> {\n    console.log('üìù Testing Real-World Scenarios...')\n    \n    const realWorldSamples = [\n      // Technical support\n      'My application keeps crashing when I try to upload large files. What could be causing this?',\n      \n      // Code review questions\n      'Should I use async/await here instead of Promises, and what are the performance implications?',\n      \n      // Planning questions\n      'What is the best approach for implementing user authentication in our new microservices architecture?',\n      \n      // Debug queries\n      'Why is this function returning undefined when I pass valid parameters?',\n      \n      // Mixed content with questions\n      'I was reading about GraphQL and I found it interesting. How does it compare to REST APIs in terms of performance?',\n      \n      // Conversational with embedded question\n      'Hey team, I hope everyone is doing well. Could someone help me understand the new deployment process?',\n      \n      // Follow-up question\n      'Thanks for explaining that. But what about error handling in this scenario?'\n    ]\n    \n    for (const sample of realWorldSamples) {\n      await this.runTest(`Real-world: \"${sample.substring(0, 50)}...\"`, async () => {\n        const result = await this.detector.detectQuestion(sample)\n        \n        // Most real-world samples should be detected as questions\n        if (!result || !result.isQuestion) {\n          console.warn(`    ‚ö†Ô∏è  Real-world question not detected: \"${sample}\"`)\n        } else {\n          console.log(`    ‚úÖ Detected as ${result.questionType} (confidence: ${result.confidence.toFixed(2)})`)\n        }\n      })\n    }\n    \n    console.log('  ‚úÖ Real-world scenario tests completed\\n')\n  }\n\n  private async runTest(testName: string, testFn: () => Promise<void>): Promise<void> {\n    this.testResults.total++\n    \n    try {\n      await testFn()\n      this.testResults.passed++\n    } catch (error) {\n      this.testResults.failed++\n      this.testResults.failures.push({\n        test: testName,\n        error: error instanceof Error ? error.message : 'Unknown error'\n      })\n      console.error(`    ‚ùå ${testName}: ${error instanceof Error ? error.message : error}`)\n    }\n  }\n\n  private printResults(): void {\n    console.log('\\n' + '='.repeat(60))\n    console.log('üìä TEST RESULTS SUMMARY')\n    console.log('='.repeat(60))\n    console.log(`Total Tests: ${this.testResults.total}`)\n    console.log(`Passed: ${this.testResults.passed} (${((this.testResults.passed / this.testResults.total) * 100).toFixed(1)}%)`)\n    console.log(`Failed: ${this.testResults.failed} (${((this.testResults.failed / this.testResults.total) * 100).toFixed(1)}%)`)\n    \n    if (this.testResults.failures.length > 0) {\n      console.log('\\n‚ùå FAILURES:')\n      this.testResults.failures.forEach((failure, index) => {\n        console.log(`${index + 1}. ${failure.test}:\\n   ${failure.error}\\n`)\n      })\n    }\n    \n    // Print detector metrics\n    const metrics = this.detector.getMetrics()\n    console.log('\\nüìà DETECTOR METRICS:')\n    console.log(`Total Analyzed: ${metrics.totalAnalyzed}`)\n    console.log(`Questions Detected: ${metrics.questionsDetected}`)\n    console.log(`Average Confidence: ${metrics.averageConfidence.toFixed(3)}`)\n    console.log(`Average Processing Time: ${metrics.averageProcessingTime.toFixed(2)}ms`)\n    console.log(`Pattern Match Hits: ${metrics.patternMatchHits}`)\n    console.log(`Semantic Analysis Hits: ${metrics.semanticAnalysisHits}`)\n    console.log(`Cache Hits: ${metrics.cacheHits}`)\n    console.log(`Errors: ${metrics.errorCount}`)\n    \n    const successRate = (this.testResults.passed / this.testResults.total) * 100\n    if (successRate >= 90) {\n      console.log('\\nüéâ EXCELLENT: Test suite passed with high success rate!')\n    } else if (successRate >= 75) {\n      console.log('\\n‚úÖ GOOD: Test suite passed with acceptable success rate.')\n    } else {\n      console.log('\\n‚ö†Ô∏è  NEEDS IMPROVEMENT: Test suite has concerning failure rate.')\n    }\n  }\n\n  async cleanup(): Promise<void> {\n    this.detector.destroy()\n  }\n}\n\n// Run tests if this file is executed directly\nif (import.meta.url === `file://${process.argv[1]}`) {\n  const runner = new TestRunner()\n  \n  process.on('SIGINT', async () => {\n    console.log('\\nüõë Test interrupted. Cleaning up...')\n    await runner.cleanup()\n    process.exit(0)\n  })\n  \n  process.on('unhandledRejection', async (error) => {\n    console.error('üí• Unhandled rejection:', error)\n    await runner.cleanup()\n    process.exit(1)\n  })\n  \n  try {\n    await runner.runAllTests()\n    await runner.cleanup()\n    process.exit(0)\n  } catch (error) {\n    console.error('üí• Test suite failed:', error)\n    await runner.cleanup()\n    process.exit(1)\n  }\n}